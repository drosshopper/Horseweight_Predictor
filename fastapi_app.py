# fastapi_app.py
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import joblib
import pandas as pd
import shap
import numpy as np
from scipy.spatial import distance
import uvicorn
from catboost import CatBoostRegressor


# ğŸ¯ FastAPIã‚¢ãƒ—ãƒª
app = FastAPI()

@app.get("/")
def read_root():
    return {"status": "ok"}

# ğŸŒ CORSï¼ˆHugging Faceã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://huggingface.co/spaces/drosshopper/horse-weight-predictor2"],  # æœ¬ç•ªç’°å¢ƒã§ã¯åˆ¶é™æ¨å¥¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
model = joblib.load("models/model_cat.pkl")  # â† Renderä¸Šã®modelsãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®
explainer = shap.Explainer(model)

# ğŸ“¥ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿å½¢å¼
class InputData(BaseModel):
    sex: int
    height: float
    waist: float
    leg: float         # â† ã“ã®1è¡Œã‚’è¿½åŠ 
    weight_age1: float
    measure: int
    daysold: int

# â–¶ï¸ é¡ä¼¼åº¦è¨ˆç®—ã«ä½¿ã†é‡ã¿ï¼ˆ10kgã«ç›¸å½“ã™ã‚‹å˜ä½å¤‰æ›ï¼‰
weights = np.array([1.0, 0.667, 2.0, 0.125])  # height, waist, leg, result

# â–¶ï¸ é‡ã¿ä»˜ããƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢é–¢æ•°
def scaled_euclidean(x, y, weights):
    diff = (x - y) * weights
    return np.sqrt(np.sum(diff**2))


# SHAPå€¤ã¨å¤‰åŒ–é‡ã‚’è¿”ã™ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/predict")
async def predict_with_shap(data: InputData, request: Request):
    input_df = pd.DataFrame([data.dict()])
    input_df_model = input_df.drop(columns=["leg"]) 
    shap_values = explainer(input_df_model)

    
    
    # SHAPã®å‡ºåŠ›ã‹ã‚‰å€¤ã‚’å–ã‚Šå‡ºã—
    base_value = shap_values.base_values[0]
    shap_contributions = shap_values.values[0].tolist()
    feature_names = shap_values.feature_names
    gain_pred = model.predict(input_df_model)[0]
    pred_weight = data.weight_age1 + gain_pred

    # âœ… é¡ä¼¼åº¦è©•ä¾¡ï¼ˆãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ + æŒ‡æ•°ã‚¹ã‚³ã‚¢ï¼‰
    input_vec = np.array([
        data.height,
        data.waist,
        pred_weight  # â† æ¨è«–å¾Œã®äºˆæ¸¬ä½“é‡
    ])
    
    # ğŸ“¦ å‚ç…§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_all = pd.read_csv("models/WeightSuggestall.csv")
    features = ["height", "waist", "result"]
    ref_df = df_all.dropna(subset=features).copy()
    
    # âœ… å…±åˆ†æ•£è¡Œåˆ—ã¯å…¨é¦¬ã§æ§‹ç¯‰
    X = ref_df[features].values
    cov_matrix = np.cov(X.T)
    inv_cov_matrix = np.linalg.inv(cov_matrix)
    
    # âœ… å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆäºˆæ¸¬ä½“é‡ï¼‰
    input_vec = np.array([data.height, data.waist, pred_weight])
    
    # âœ… å…¨é¦¬ã«è·é›¢ãƒ»ã‚¹ã‚³ã‚¢ã‚’ä»˜ä¸
    ref_df["distance"] = [mahalanobis_dist(input_vec, row, inv_cov_matrix) for row in X]
    beta = 0.4
    ref_df["score"] = 100 * np.exp(-beta * ref_df["distance"])
    
    # âœ… é‡è³é¦¬ã®ã¿æŠ½å‡ºã—ã¦ä¸Šä½3é ­ã‚’è¿”ã™
    graded_df = ref_df[ref_df["graded_winner"] == 1].copy()
    
    top_matches = []
    for _, row in graded_df.sort_values("score", ascending=False).head(3).iterrows():
        top_matches.append({
            "name": row.get("name", "ä¸æ˜"),
            "distance": round(row["distance"], 3),
            "score": round(row["score"], 1),
            "features": {
                "height": row["height"],
                "waist": row["waist"],
                "result": row["result"]
            },
            "graded_titles": [
                win for win in [row.get("win1"), row.get("win2"), row.get("win3")] if pd.notna(win)
            ]
        })








    return {
        "gain_pred": gain_pred,
        "base_value": base_value,
        "contributions": shap_contributions,
        "features": feature_names,
        "top_matches": top_matches  # ğŸ‘ˆ åé¦¬é¡ä¼¼åº¦TOP3ã‚’è¿½åŠ 

    }
